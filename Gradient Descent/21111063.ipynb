{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "tOYiB1ZmLOHU"
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX2lCS1_LOHX"
   },
   "source": [
    "# Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIKyIJIXLOHZ"
   },
   "source": [
    "## a(i) Use gradient descent to find minima for $f(x) = x^2 + 3x + 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6XWX8ZGLOHZ"
   },
   "source": [
    " $$ g = \\frac{\\partial f(x)}{\\partial x} = \\frac{\\partial (x^2 + 3x + 4)}{\\partial x} = 2x + 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "AGl0i3xrLOHZ"
   },
   "outputs": [],
   "source": [
    "def gradient( x ):\n",
    "    return 2*x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "5Nl64kvnLOHa"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, init_, learn_rate, n_iter = 5000, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(abs(delta)<=tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcraDpfLLOHa",
    "outputId": "84a79bd9-7871-4f5b-a818-92a317650e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point at which function attains minimum value is  -1.5\n"
     ]
    }
   ],
   "source": [
    "minx = gradient_descent(gradient, 4 , 0.05 )\n",
    "print(\"The point at which function attains minimum value is \",minx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=x^2+3x+4$ <br> \n",
    "$f'(x)={2}x+3$ <br>\n",
    "$f'(x)=0 {\\;}{\\;} when{\\;}{\\;} x=-1.5$ <br>\n",
    "$f''(x)=2 >0{\\;}{\\;} when{\\;}{\\;} x=-1.5$<br>\n",
    "$\\therefore$ It is postive semidefinite and hence it is the minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8O_SiqLLOHb",
    "outputId": "cf5c0be0-d348-49f7-c108-c371f4d7eb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of function is  1.75\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*x + 3*x + 4\n",
    "print(\"Minimum value of function is \",f(minx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGjbE_pxLOHc"
   },
   "source": [
    "## a(ii) Use gradient descent to find minima for $ x^4 - 3x^2 + 2x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgwkS7TgLOHc"
   },
   "source": [
    "$$ g = \\frac{\\partial f(x)}{\\partial x} = \\frac{\\partial (x^4 - 3x^2 + 2x)}{\\partial x} = 4x^3 - 6x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "MZkaH1GWLOHd"
   },
   "outputs": [],
   "source": [
    "def gradient( x ):\n",
    "    return 4*(x**3) - 6*x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "vm7jpL1xLOHd"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, init_, learn_rate, n_iter = 5000, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(abs(delta)<=tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jFDeWLLLOHd",
    "outputId": "3bd9cf90-272b-47dd-c1a8-f2f4aa1473af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point at which function attains minimum value is  -1.366\n"
     ]
    }
   ],
   "source": [
    "minx = gradient_descent( gradient, 0 , 0.01 )\n",
    "print(\"The point at which function attains minimum value is \",minx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=x^4-3x^2+2x $ <br> \n",
    "$f'(x)={4}x^3-{6}x+2$ <br>\n",
    "$f'(x)=4x^3-6x+2=0 \\Rightarrow  x = 1, -1/2\\pm \\sqrt3/2$<br>\n",
    "$f''(1)=6 >0$  <br>\n",
    "$f''(-1/2+\\sqrt3/2) = -4.39 < 0 \\Rightarrow Rejected{\\;}as{\\;}it{\\;}cannot{\\;}be{\\;}minima$<br>\n",
    "$f''(-1/2-\\sqrt3/2) = 16.39 > 0 $<br>\n",
    "$f(1)=0 \\Rightarrow Local{\\;}minima$<br>\n",
    "$f(-1/2-\\sqrt3/2)=f(-1.366)=-4.8480 \\Rightarrow Global{\\;}minima$<br>\n",
    "$\\therefore$ It is postive semidefinite and hence it is the minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0MxCpGtLOHe",
    "outputId": "0b8016d1-ff1c-4ccd-9156-4329a109c937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of function is  -4.848076206064\n"
     ]
    }
   ],
   "source": [
    "def f( x ):\n",
    "    return (x**4) - 3*(x**2) + 2*x\n",
    "print(\"Minimum value of function is \",f(minx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf-sbPXjLOHe"
   },
   "source": [
    "# 1(b).  Gradient function to calculate gradients for a linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7sDdZO8LOHe"
   },
   "source": [
    "$$y = aX + b$$\n",
    "$$L = \\frac{\\sum{(y - y\\_predict)}^2}{N}$$<br>\n",
    "$$\\frac{\\partial (L)}{\\partial a} = -\\frac{2}{N}*\\sum(X(y - y\\_predict))$$<br>\n",
    "$$\\frac{\\partial (L)}{\\partial b} = -\\frac{2}{N}*\\sum(y - y\\_predict) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient : the direction of the steepest change in function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "_UJAjgTpLOHf"
   },
   "outputs": [],
   "source": [
    "def gradient( X, y, y_predict):\n",
    "    #length of the dataset .ie. No. of instances\n",
    "    n = X.shape[0] \n",
    "    gradient_b = -(2/n)*sum(y - y_predict) # As per the gradient formula derived\n",
    "    gradient_a = -(2/n)*np.dot(X.T,(y - y_predict)) # As per the gradient formula derived\n",
    "    return gradient_b, gradient_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVFLuIzhLOHf"
   },
   "source": [
    "# 1(c). Use gradient descent to find the optimal parameters relating X with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "Rnz3bPL9LOHf"
   },
   "outputs": [],
   "source": [
    "# Dataset generation\n",
    "np.random.seed(0)\n",
    "X = 2.5*np.random.randn(10000)+1.5 \n",
    "res=1.5*np.random.randn(10000) \n",
    "y = 2+0.3*X + res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Algorithm <br>\n",
    "$\\bf for{\\;}k=0,1,2\\dots (or{\\;}until{\\;}convergence)$ <br>\n",
    "$\\bf do $<br> \n",
    "${\\;}{\\;}{\\;}{\\;}{\\;}{\\;}g_k = \\nabla f(x_k)$<br>\n",
    "${\\;}{\\;}{\\;}x_{k+1} = x_k - t_k g_k$ <br>\n",
    "$\\bf end{\\;}for$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "PdY1Jj8XLOHg"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learn_rate = 0.05, max_iter = 1000, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    \n",
    "    gradient_a = 0 #gradient of a\n",
    "    gradient_b = 0 ##gradient of b\n",
    "    \n",
    "    \n",
    "    for i in range( max_iter ):\n",
    "        y_predict = a*X + b\n",
    "        g = gradient(X, y , y_predict) #as defined in 1(b)\n",
    "        \n",
    "        gradient_b = g[0]\n",
    "        gradient_a = g[1]\n",
    "        \n",
    "#         print('gradient_a: ', gradient_a, '    gradient_b: ', gradient_b)\n",
    "        \n",
    "        delta_a = -1*learn_rate*(gradient_a)\n",
    "        delta_b = -1*learn_rate*(gradient_b)\n",
    "        \n",
    "        if (np.all(np.abs(delta_a) <= tol) & np.all(np.abs(delta_b) <= tol)):\n",
    "            break\n",
    "        a = a + delta_a\n",
    "        b = b + delta_b\n",
    "    return (round(a,2),round(b,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "ZZyFmxFjLOHg"
   },
   "outputs": [],
   "source": [
    "timegd = time.time()\n",
    "a_calculated, b_calculated = gradient_descent(X, y, 0.005, 1000, 1e-06)\n",
    "timegd = time.time() - timegd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNkeWfbsLOHh",
    "outputId": "a68a4025-ed39-4218-9d82-98bb92f1a75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value using gradient descent linear algorithm \n",
      "Optimal parameters relating X and y are: a= 0.3  b= 2.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted value using gradient descent linear algorithm \")\n",
    "print(\"Optimal parameters relating X and y are: a=\",a_calculated,\" b=\",b_calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbF4vXE2LOHh"
   },
   "source": [
    "## 1.(d) Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $\\mathbf{w}$ as $\\mathbf{w^0}$<br>\n",
    "Set a learning rate ${\\mathbf{n}}_t$<br>\n",
    "Choose a random number B uniformly randomnly, and select $\\{i_1, i_2,...,i_B\\}\\in \\{1,2,...N\\}$<br> \n",
    "$\\bf for{\\;}i=0,1,2\\dots (or{\\;}until{\\;}convergence)$ <br>\n",
    "$    {\\;}{\\;}{\\;}{\\;}{\\;}    {\\mathbf{g}} =\\sum_{b=1}^B \\mathbf{g}_{i_b}$<br>\n",
    "${\\;}{\\;}{\\;}{\\;}{\\;}\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} -{\\mathbf{n}_t}{\\mathbf{g}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "Baf5VBSaLOHh"
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, batchsize, learn_rate = 0.01, max_iter = 5000, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    \n",
    "    gradient_a = 0\n",
    "    gradient_b = 0\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    iterations = 0\n",
    "    for i in range( max_iter ):\n",
    "        iterations = iterations + 1\n",
    "\n",
    "        B = np.random.randint(0,n-batchsize-1) #B: block size taken from [B] to [B+batchsize]\n",
    "        X_temp = X[B :B + batchsize] #current batch sample\n",
    "        y_temp = y[B :B + batchsize] #current batch output\n",
    "        \n",
    "        n_temp = X_temp.shape[0]  #size of current batch taken\n",
    "        y_predict = a*X_temp + b  #predicted value of current batch\n",
    "        \n",
    "        g = gradient(X_temp, y_temp , y_predict) #as defined in 1(b)\n",
    "        \n",
    "        gradient_b = g[0]\n",
    "        gradient_a = g[1]\n",
    "        \n",
    "        delta_a = -1*learn_rate*(gradient_a)\n",
    "        delta_b = -1*learn_rate*(gradient_b)\n",
    "        \n",
    "        if (np.all(np.abs(delta_a) <= tol) & np.all(np.abs(delta_b) <= tol)):\n",
    "            break\n",
    "        a = a + delta_a\n",
    "        b = b + delta_b\n",
    "    return (round(a,2),round(b,2),iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZRIx1T2LOHi",
    "outputId": "d99f47e4-524c-47e5-be8c-1d056f9ddc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch Gradient Descent\n",
      "0.3 2.01 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Minibatch Gradient Descent\")\n",
    "\n",
    "a_predicted, b_predicted, iterations = minibatch_gradient_descent(X, y, 1024)\n",
    "\n",
    "print(a_predicted, b_predicted, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvsWpydoLOHi"
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $\\mathbf{w}$ as $\\mathbf{w^0}$<br>\n",
    "Set a learning rate ${\\mathbf{n}}_t$<br>\n",
    "$\\bf for{\\;}i=0,1,2\\dots (or{\\;}until{\\;}convergence)$ <br>\n",
    "\n",
    "$    {\\;}{\\;}{\\;}{\\;}{\\;}$Choose a random number $ k\\in \\{1,2,...N\\}$<br>\n",
    "$    {\\;}{\\;}{\\;}{\\;}{\\;}    {\\mathbf{g}} =\\mathbf{g}_{k}$<br>\n",
    "${\\;}{\\;}{\\;}{\\;}{\\;}\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} -{\\mathbf{n}_t}{\\mathbf{g}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "zJFeVRfHLOHi"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learn_rate = 0.05, max_iter = 10000, tol = 1e-06):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    \n",
    "    gradient_a = 0\n",
    "    gradient_b = 0\n",
    "    batchsize = 1\n",
    "    n = X.shape[0]\n",
    "    iterations = 0\n",
    "    for i in range( max_iter ):\n",
    "        iterations = iterations + 1\n",
    "\n",
    "        B = np.random.randint(0,n-1) #B: block size taken from [B] to [B+batchsize]\n",
    "        X_temp = X[B :B + batchsize] #current batch sample\n",
    "        y_temp = y[B :B + batchsize] #current batch output\n",
    "        \n",
    "        n_temp = X_temp.shape[0]  #size of current batch taken\n",
    "        y_predict = a*X_temp + b  #predicted value of current batch\n",
    "        \n",
    "        g = gradient(X_temp, y_temp , y_predict) #as defined in 1(b)\n",
    "        \n",
    "        gradient_b = g[0]\n",
    "        gradient_a = g[1]\n",
    "        \n",
    "        delta_a = -1*learn_rate*(gradient_a)\n",
    "        delta_b = -1*learn_rate*(gradient_b)\n",
    "        \n",
    "        if (np.all(np.abs(delta_a) <= tol) & np.all(np.abs(delta_b) <= tol)):\n",
    "            break\n",
    "        a = a + delta_a\n",
    "        b = b + delta_b\n",
    "    return (round(a,2),round(b,2),iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyEkxssCLOHi"
   },
   "source": [
    "### Since stochastic gradient descent gives random outcomes. So I am trying to run the algorithm multiple times and find the optimal execution when it converges and note the time taken in such a case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByKGbIKnLOHj",
    "outputId": "2abf11c9-19dc-4574-f29d-5f2d7ef089ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Best Outcome\n",
      "a= 0.3 ,b= 1.99 iterations= 1000 ,time taken= 0.09245705604553223\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    timeSGD = time.time()\n",
    "    a_predicted, b_predicted, iterations = stochastic_gradient_descent(X, y, 0.005, 1000, 1e-06)\n",
    "    timeSGD = time.time() - timeSGD\n",
    "    if ((a_predicted in [0.29, 0.3, 0.31]) and (b_predicted in [1.99, 2.0, 2.01])):\n",
    "        break\n",
    "print(\"Stochastic Gradient Descent Best Outcome\")\n",
    "print(\"a=\",a_predicted,\",b=\", b_predicted,\"iterations=\", iterations,\",time taken=\",timeSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQCiG9a9LOHj"
   },
   "source": [
    "# 1.(e)  Time performance on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOKS8ZLULOHj"
   },
   "source": [
    "## Performance Estimation\n",
    "#### Here I estimate the performance of MiniBatch Gradient Descent, Stochastic Gradient (SGD) and Gradient Descent (GD) and thereafter compare their runtimes.\n",
    "#### I have taken batch sizes in power of 2, -ie- 1,2,4,8,16,32,64... because it is computationally faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "fAOEWgPlLOHj"
   },
   "outputs": [],
   "source": [
    "for i in range(0,12):\n",
    "    batchsize = (2**i)\n",
    "    timeMGD = time.time()\n",
    "    optimal_batchsize = batchsize\n",
    "    a1,b1,itr1 = minibatch_gradient_descent(X,y,batchsize,0.005, 1000,1e-06) #(X, y, batchsize, learn_rate = 0.01, max_iter = 5000, tol = 1e-04)\n",
    "    timeMGD = time.time() - timeMGD\n",
    "    if ((a1 in [0.29, 0.3, 0.31]) and (b1 in [1.99, 2.0, 2.01])):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGvQRIItLOHk",
    "outputId": "d51bd50e-9302-4e35-a0d5-fe47aaf631d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by Minibatch Gradient Descent: 0.11353182792663574\n",
      "Time taken by Batch Gradient Descent: 5.345445394515991\n",
      "Time taken by Stochastic Gradient Descent: 0.09245705604553223\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken by Minibatch Gradient Descent:\",timeMGD)\n",
    "print(\"Time taken by Batch Gradient Descent:\",timegd)\n",
    "print(\"Time taken by Stochastic Gradient Descent:\",timeSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lxOzIP5LOHk"
   },
   "source": [
    "## Performance Interpretation\n",
    "### (i) It is clear that Stochastic Gradient Descent (SGD) performs the best among all of them. It takes the minimum time for running on the given data set. This is so because it takes only a single data point for gradient update. The quantified proof is given below in the runtime comparison of SGD with all others.\n",
    "#### $$Batch\\; Gradient\\; Descent > MiniBatch\\; Gradient\\; Descent > Stochastic\\; Gradient\\; Descent$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6uBhnnrLOHl",
    "outputId": "0bd5c126-a029-4f32-c71f-454a05ad4bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Minibatch and Batch GD:\n",
      "Time taken by Minibatch Gradient Descent: 0.11353182792663574\n",
      "Time taken by Batch Gradient Descent: 5.345445394515991\n",
      "% reduction in time by Minibatch over Batch GD:  97.87610162395241\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison between Minibatch and Batch GD:\")\n",
    "print(\"Time taken by Minibatch Gradient Descent:\",timeMGD)\n",
    "print(\"Time taken by Batch Gradient Descent:\",timegd)\n",
    "print(\"% reduction in time by Minibatch over Batch GD: \", (timegd-timeMGD)*100/timegd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8_FGNviLOHl",
    "outputId": "4edfdb92-9ae2-428a-a30e-67b22a2c7838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Batch Gradient Descent and SGD:\n",
      "Time taken by Batch Gradient Descent: 5.345445394515991\n",
      "Time taken by Stochastic Gradient Descent: 0.09245705604553223\n",
      "% reduction in time by SGD over Batch GD:  98.27035823543561\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison between Batch Gradient Descent and SGD:\")\n",
    "print(\"Time taken by Batch Gradient Descent:\",timegd)\n",
    "print(\"Time taken by Stochastic Gradient Descent:\",timeSGD)\n",
    "print(\"% reduction in time by SGD over Batch GD: \", (timegd-timeSGD)*100/timegd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWJqMX8tYXPW",
    "outputId": "a7351164-9b8c-4767-f7af-7987c0a638e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Minibatch Gradient Descent and SGD:\n",
      "Time taken by Minibatch Gradient Descent: 0.11353182792663574\n",
      "Time taken by Stochastic Gradient Descent: 0.09245705604553223\n",
      "% reduction in time by SGD over Minibatch Gradient Descent:  18.562875508991215\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison between Minibatch Gradient Descent and SGD:\")\n",
    "print(\"Time taken by Minibatch Gradient Descent:\",timeMGD)\n",
    "print(\"Time taken by Stochastic Gradient Descent:\",timeSGD)\n",
    "print(\"% reduction in time by SGD over Minibatch Gradient Descent: \", (timeMGD-timeSGD)*100/timeMGD)\n",
    "#Note: If the percentage turns out negative, please run the entire notebook again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Optimal batch size for Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal batchsize =  64\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal batchsize = \",optimal_batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Okq2L_4LOHl"
   },
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{Let{\\;}us{\\;}define{\\;}Events:}$<br>\n",
    "$\\bullet cold$ : person has cold<br>\n",
    "$\\bullet lung$ : person has lung disease<br>\n",
    "$\\bullet cough$ : person has cough<br>\n",
    "$\\bullet smoke$ : person smokes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJw_9z7yLOHl"
   },
   "source": [
    "## 2(a) Calculate the probability that someone has both cold and a fever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7JCi4l2LOHm"
   },
   "source": [
    "Given data:  \n",
    "$(i){\\;}{\\;} P(cold)=0.02$<br>\n",
    "$(ii){\\;}{\\;} P(fever | cold)=0.307$<br><br>\n",
    "Formula Used:<br>\n",
    "Conditional probability formula  <br>\n",
    "$P(A|B)=\\frac{P(A\\cap B)}{P(B)} $\n",
    "where A, B are any 2 events.<br>\n",
    "By cross multiplication, we get <br>\n",
    "$P(A\\cap B)= P(A|B)*P(B)$<br><br>\n",
    "Solution:<br>\n",
    "Substitute A = fever and B = cold in the above formula, we get <br>\n",
    "$P(fever \\cap cold) = P(cold)* P(fever|cold)$ <br>\n",
    "$P(fever \\cap cold) = 0.02 * 0.307 $<br>\n",
    "$P(fever \\cap cold) = 0.00614 $<br>\n",
    "$ \\therefore Answer = P(fever \\cap cold) = 0.00614 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoF3KDyDLOHm"
   },
   "source": [
    "## 2(b)  Calculate the probability that someone who has a cough has a cold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX8lGxMeLOHm"
   },
   "source": [
    "$\\mathbf{To{\\;}find{\\;}:}$<br>\n",
    "$P(cough|cold) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkNH1iSiLOHm"
   },
   "source": [
    "$\\mathbf{Formula{\\;}used:}$<br>\n",
    "$$ Bayes'{\\;}{\\;}Theorem{\\;}P(cold|cough) =\\frac{P(cold \\cap cough)}{P(cough)}{\\;}{\\;}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6S6lOrYLOHn"
   },
   "source": [
    "$\\mathbf{Solution :}$<br><br>\n",
    "By theorem of Total Probability<br>\n",
    "$P(lung{\\;}) = P(lung\\cap smoke) + P(lung\\cap smoke^c)$<br> \n",
    "$P(lung{\\;}) = {P(smoke)}{ P(lung\\;|smoke)} + P{(smoke^c)}{P(lung\\;|smoke)}$<br>\n",
    "$P(lung{\\;}) = 0.2 * 0.1009 + 0.8 * 0.001 $<br>\n",
    "$P(lung{\\;}) = 0.02018 + 0.0008 $<br>\n",
    "$P(lung{\\;}) =0.02098  \\ldots (1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqALuNihLOHn"
   },
   "source": [
    "Probability of Intersection of 2 or more events <br>\n",
    "$P(A\\cap B \\cap C) =P((A\\cap B)\\cap C) =P(C|(A\\cap B)) P(A\\cap B) = P(C|(A\\cap B))({P(B|A)}{P(A)} ={P(A)}{P(B|A)}{P(C|A\\cap B)} \\ldots (2)$<br>\n",
    "$P(cough)=P(cough\\cap lung\\cap cold) + P(cough\\cap lung\\cap cold^c)+ P(cough\\cap lung^c\\cap cold)+ P(cough\\cap lung^c \\cap cold^c) \\ldots (3)$<br><br>\n",
    "Evaluating the expression on RHS by using eqn (2) on each term of RHS <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho36HDoBLOHn"
   },
   "source": [
    "$P(cough\\cap lung\\cap cold) = P(cough |(lung\\cap cold) * P(lung|cold) * P(cold)$<br>\n",
    "$P(cough\\cap lung\\cap cold) = 0.0525 * 0.02098 * 0.02$<br>\n",
    "$P(cough\\cap lung\\cap cold) = 0.000315749 \\ldots\\ldots (4)$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v24lNDnLLOHn"
   },
   "source": [
    "$P(cough\\cap lung\\cap cold^c) = P(cough |(lung\\cap cold^c) * P(lung|cold^c) * P(cold^c)$<br>\n",
    "$P(cough\\cap lung\\cap cold^c) = 0.505 * 0.02098 * 0.98$<br>\n",
    "$P(cough\\cap lung\\cap cold^c) = 0.010383002 \\ldots\\ldots (5)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT_q-pZ2LOHn"
   },
   "source": [
    "$P(cough\\cap lung^c\\cap cold) = P(cough |(lung^c\\cap cold) * P(lung^c|cold) * P(cold)$<br>\n",
    "$P(cough\\cap lung^c\\cap cold) = 0.505 * 0.97902 * 0.02$<br>\n",
    "$P(cough\\cap lung^c\\cap cold) = 0.00988102 \\ldots\\ldots (6)$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhUF2RlbLOHo"
   },
   "source": [
    "$P(cough\\cap lung^c\\cap cold^c) = P(cough |(lung^c\\cap cold^c) * P(lung^c|cold^c) * P(cold)$<br>\n",
    "$P(cough\\cap lung^c\\cap cold^c) = 0.01 * 0.97902 * 0.98$<br>\n",
    "$P(cough\\cap lung^c\\cap cold^c) = 0.00988102 \\ldots\\ldots (7)$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NauW9lR_LOHo"
   },
   "source": [
    "Substituting the values from equation (4),(5),(6),(7) into (3),<br>\n",
    "$P(cough) = 0.000315749  + 0.010383002 + 0.00988102 + 0.00988102 $<br>\n",
    "$P(cough) = 0.030181249 \\ldots\\ldots(8)$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD_zewqcLOHo"
   },
   "source": [
    "$$ P(cold|cough) =\\frac{P(cold \\cap cough)}{P(cough)}{\\;}{\\;}$$<br>\n",
    "$$ P(cold|cough) = P(cold\\cap cough\\cap lung) + P(cold\\cap cough\\cap lung^c)$$<br>\n",
    "$$ P(cold|cough) = P(cough |(lung\\cap cold)*P(lung|cold)*P(cold) + P(cough|(lung^c\\cap cold))*P(lung^c|cold)*P(cold)$$<br>\n",
    "$$ P(cold|cough) = 0.0525 * 0.02098 * 0.02 + 0.505 * 0.97902 * 0.02 $$<br>\n",
    "$$ P(cold|cough) = 0.000315749 + 0.00988102 $$<br>\n",
    "$$ P(cold|cough) = 0.3380857764 $$<br>\n",
    "$$ \\therefore Answer{\\;}= P(cold|cough) = 0.3380857764 $$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSgeObFhLOHo"
   },
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mBMOWnjLOHp"
   },
   "source": [
    "## Derive the MLE for the parameters of a k-sided multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhQGBe5CLOHp"
   },
   "source": [
    "\n",
    "Let \n",
    "<br>\n",
    "$n = $ No. of times we repeat the experiement \n",
    "<br>\n",
    "$k = $ No. of possible outcomes for each individual experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dPcstGfLOHp"
   },
   "source": [
    "$Y$ :  Random variable denoting the outcome for each possible outcomes  \n",
    "$Y : {Y_1 , Y_2, Y_3 \\ldots Y_k}$ \n",
    "<br>\n",
    "where \n",
    "<br>\n",
    "$Y_1$ denotes first possible outcome<br>\n",
    "$Y_2$ denotes second possible outcome<br>\n",
    "$\\vdots$<br>\n",
    "$Y_k$ denotes kth possible outcome<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KnMyooLLOHp"
   },
   "source": [
    "$$P(Y_1=x_1, Y_2=x_2,...,Y_k=x_k | {\\theta_1^{x_1}}{\\theta_2^{x_2}}\\ldots{\\theta_k^{x_k}})=\\bigl({\\frac{n!}{{x_1!}{x_2!}\\ldots{x_k!}}}\\bigr) {{\\theta_1^{x_1}}{\\theta_2^{x_2}}\\ldots{\\theta_k^{x_k}}}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_mQtiPBLOHp"
   },
   "source": [
    "where <br>\n",
    "$x_1$ : No. of times first outcome occured <br>\n",
    "$x_2$ : No. of times second outcome occured <br>\n",
    "$x_3$ : No. of times third outcome occured <br>\n",
    "$\\vdots$<br>\n",
    "$x_k$ : No. of times kth outcome occured <br>\n",
    "$$\\therefore x_1 + x_2 + x_3 + \\cdots + x_k = n$$ <br>\n",
    "$$\\sum_{i=0}^n x_i = {n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CYbIFlELOHq"
   },
   "source": [
    "and <br>\n",
    "$\\theta_1$ : probability of having first outcome <br>\n",
    "$\\theta_2$ : probability of having second outcome <br>\n",
    "$\\theta_3$ : probability of having third outcome <br>\n",
    "$\\vdots$ <br>\n",
    "$\\theta_k$ : probability of having kth outcome <br>\n",
    "$$\\therefore \\theta_1 + \\theta_2 + \\theta_3 + \\cdots + \\theta_k = 1$$ <br>\n",
    "$$\\sum_{i=1}^k \\theta_i = {1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0igq_4LOHq"
   },
   "source": [
    "Therefore the likelihood <br> \n",
    "$ L(\\theta) $<br>\n",
    "$= L(x_1, x_2, \\ldots,x_n | \\theta) $<br>\n",
    "$= L(x_1, x_2, \\ldots,x_n | {\\theta_1}{\\theta_2}\\ldots{\\theta_k}) $<br>\n",
    "\n",
    "$=\\bigl({\\frac{n!}{{x_1!}{x_2!}\\ldots{x_k!}}}\\bigr) {{\\theta_1^{x_1}}{\\theta_2^{x_2}}\\ldots{\\theta_k^{x_k}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dj5rKFOpLOHq"
   },
   "source": [
    "Let us define a new function <br> $ LL(\\theta) = log {\\;}L(\\theta) $<br>\n",
    "Substituting the value from above,<br>\n",
    "$ LL(\\theta) = \\log {\\;} \\Bigl(\\bigl({\\frac{n!}{{x_1!}{x_2!}\\ldots{x_k!}}}\\bigr) {{\\theta_1^{x_1}}{\\theta_2^{x_2}}\\ldots{\\theta_k^{x_k}}}\\Bigr) $ <br>\n",
    "Expanding logarithm on each term <br>\n",
    "$ LL(\\theta) = \\log {\\;} ( n! \\theta_1^{x_1} \\theta_2^{x_2} \\ldots \\theta_k^{x_k})  - \\log (x_1! x_2! \\ldots x_k!)$ <br>\n",
    "$ LL(\\theta) = \\log{\\;}(n!) -  \\sum_{i=1}^k x_i \\log\\theta_i   - \\log \\sum_{i=1}^k (x_i!)$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS_VV8oyLOHq"
   },
   "source": [
    "Adding a regularization parameter $\\alpha$, to avoid $\\theta$ taking too large values <br>\n",
    "$\\alpha$: Langrangian parameter for regularization <br>\n",
    "Now, the equation becomes $ Regularized{\\;} Log{\\;} Likelihood{\\;} \\theta$ abbreviated as $RLL(\\theta,\\alpha)$<br>\n",
    "$ RLL(\\theta,\\alpha) = LL(\\theta) + \\alpha ( 1 - \\sum_{i=1}^k \\theta_i )$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR7cuW5wLOHr"
   },
   "source": [
    "To find the maximum of a function, we differentiate it with respect to $\\theta$ and equating it to zero <br>\n",
    "$$ \\frac{\\partial RLL(\\theta,\\alpha) } {\\partial \\theta} = \\Bigl( \\frac{\\partial RLL(\\theta,\\alpha) } {\\partial \\theta_1}, \\frac{\\partial RLL(\\theta,\\alpha)}{\\partial \\theta_2},\\ldots\\frac{\\partial RLL(\\theta,\\alpha)}{\\partial \\theta_k}\\Bigr)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLUtFlkcLOHr"
   },
   "source": [
    "$$\\frac{\\partial RLL(\\theta,\\alpha) } {\\partial \\theta_1} =\\frac{\\partial \\log{\\;}(n!) } {\\partial \\theta_1} + \\frac{\\partial\\sum_{i=1}^k x_i \\log\\theta_i}{\\partial \\theta_1} - \\frac{\\partial\\log \\sum_{i=1}^k (x_i!)}{\\partial \\theta_1} + \\frac{\\partial(\\alpha ( 1 - \\sum_{i=1}^k \\theta_i ))}{\\partial \\theta_1} $$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwHkz-ayLOHr"
   },
   "source": [
    "On solving the above equation, we get <br>\n",
    "$$\\frac{\\partial RLL(\\theta,\\alpha)}{\\partial \\theta_1} =\\frac{x_1}{\\theta_1} - \\alpha = 0 $$\n",
    "$Similarly$<br>\n",
    "$$\\frac{\\partial RLL(\\theta,\\alpha)}{\\partial \\theta_2} =\\frac{x_2}{\\theta_2} - \\alpha = 0 $$\n",
    "$$\\vdots$$\n",
    "$$\\frac{\\partial RLL(\\theta,\\alpha)}{\\partial \\theta_k} =\\frac{x_k}{\\theta_k} - \\alpha = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G7eTDzjLOHr"
   },
   "source": [
    "$$\\therefore\\theta_1 = \\frac{x_1}{\\alpha},$$<br> \n",
    "$Similarly$<br>\n",
    "$$ \\theta_2 = \\frac{x_2}{\\alpha},$$<br>\n",
    "$$ \\vdots $$<br>\n",
    "$$ \\theta_k = \\frac{x_k}{\\alpha}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgIodDLTLOHr"
   },
   "source": [
    "$$ \\therefore \\sum_{i=1}^k \\theta_i = 1 \\Rightarrow \\alpha = n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z14M-_SALOHs"
   },
   "source": [
    "$$ \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta^2} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta_1^2} & \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial\\theta_1\\partial\\theta_2 } & \\ldots & \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial\\theta_1\\partial\\theta_k} \\\\\n",
    "\\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta_2\\partial\\theta_1} & \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial\\theta_2^2 } & \\ldots & \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial\\theta_2\\partial\\theta_k}\\\\\n",
    "\\vdots & & \\ddots \\\\\n",
    "\\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta_k\\partial\\theta_1} & \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta_k\\partial\\theta_2} & \\ldots & \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial\\theta_k^2}\\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko59N91GLOHs"
   },
   "source": [
    "$$ \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta^2} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial (\\frac{x_1}{\\theta_1} - \\alpha)}{\\partial \\theta_1} & \\frac{\\partial(\\frac{x_2}{\\theta_2} - \\alpha)}{\\partial\\theta_1} & \\ldots & \\frac{\\partial(\\frac{x_k}{\\theta_k} - \\alpha)}{\\partial\\theta_1} \\\\\n",
    "\\frac{\\partial(\\frac{x_1}{\\theta_1} - \\alpha)}{\\partial \\theta_2} & \\frac{\\partial(\\frac{x_2}{\\theta_2} - \\alpha)}{\\partial\\theta_2 } & \\ldots & \\frac{\\partial(\\frac{x_k}{\\theta_k} - \\alpha)}{\\partial\\theta_2}\\\\\n",
    "\\vdots & & \\ddots \\\\\n",
    "\\frac{\\partial(\\frac{x_1}{\\theta_1} - \\alpha)}{\\partial \\theta_k} & \\frac{\\partial(\\frac{x_2}{\\theta_2} - \\alpha)}{\\partial \\theta_k} & \\ldots & \\frac{\\partial(\\frac{x_k}{\\theta_k} - \\alpha)}{\\partial\\theta_k}\\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s6L2uK_LOHs"
   },
   "source": [
    "$$ \\frac{\\partial^2 RLL(\\theta,\\alpha)}{\\partial \\theta^2} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{-x_1}{\\theta_1^2} & 0 & \\ldots & 0 \\\\\n",
    "0 & \\frac{-x_2}{\\theta_2^2} & \\ldots & 0 \\\\\n",
    "\\vdots & & \\ddots \\\\\n",
    "0 & 0 & \\ldots & \\frac{-x_k}{\\theta_k^2} \\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVlLs-lJLOHs"
   },
   "source": [
    "$$ \\frac{-x_i}{\\theta_i^2} \\lt 0 {\\;} $$\n",
    "$$\\forall{\\;}1\\leq i\\leq k$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axMJ2C9pLOHs"
   },
   "source": [
    "Since the eigen values of the above matrix are negative. Hence, it is Negative Semi-Definite Matrix.<br>\n",
    "Therefore, the likelihood function is going to give us a Maximum.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6sFWQDxLOHt"
   },
   "source": [
    "$$Thus,{\\;}the{\\;}Maximum{\\;}Likelihood{\\;}Estimate{\\;}(MLE){\\;}of{\\;}k-fold{\\;}multinomial{\\;}distribution{\\;}is{\\;}$$\n",
    "$$\\therefore\\theta_1 = \\frac{x_1}{n}, \\theta_2 = \\frac{x_2}{n}, \\ldots \\theta_k = \\frac{x_k}{n}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TKHdDpqLOHt",
    "outputId": "9d6bf830-15ef-4ac6-9c21-1a3fb6d05379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of assignment\n"
     ]
    }
   ],
   "source": [
    "print(\"End of assignment\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UbF4vXE2LOHh",
    "YvsWpydoLOHi",
    "HyEkxssCLOHi",
    "kJw_9z7yLOHl",
    "YoF3KDyDLOHm"
   ],
   "name": "Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
